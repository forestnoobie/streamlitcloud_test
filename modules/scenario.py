import argparse
import itertools
import os
import random
import re
import requests
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path

import jinja2
import numpy as np
import pandas as pd
import streamlit as st
import yaml
from bs4 import BeautifulSoup
from openai import OpenAI
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from modules.utils import (
    get_naverlinks, fetch_article_details, get_jinga,
    get_study, get_study_format, get_output_format,
    get_shorturl, check_date
)

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, os.path.dirname(SCRIPT_DIR))


prompt_templates = {

    "system" : "You are a helpful journalist assistant designed to output JSON.",
        
    "digital" : 
    """ë‹¹ì‹ ì˜ ì—…ë¬´ëŠ” ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ë¥˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 
    ì£¼ì–´ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì•„ë˜ ì¡°ê±´ì— ë”°ë¼ ë””ì§€í„¸ ê´€ë ¨ ê¸°ì‚¬ë©´ 1, ì•„ë‹ˆë©´ 0ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
    ë‹µë³€ì€ "ë””ì§€í„¸"ì´ë¼ëŠ” í‚¤ ê°’ìœ¼ë¡œ JSON í˜•íƒœë¡œ ì¶”ì¶œí•˜ì„¸ìš”. 
    "You are a helpful assistant designed to output JSON."

    ë””ì§€í„¸ ê´€ë ¨ ê¸°ì‚¬ë¡œ íŒë‹¨í•˜ë ¤ë©´ ë‹¤ìŒ ê¸°ì¤€ ì¤‘ í•˜ë‚˜ ì´ìƒì„ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤:

    1. ê¸ˆìœµ ê¸°ìˆ (FinTech)ì´ë‚˜ ê´€ë ¨ ë””ì§€í„¸ ì†”ë£¨ì…˜ì— ëŒ€í•´ ë‹¤ë£¨ê³  ìˆëŠ”ê°€? (ì˜¨ë¼ì¸ ë±…í‚¹, ëª¨ë°”ì¼ ê²°ì œ ì‹œìŠ¤í…œ, ë””ì§€í„¸ ìì‚° ê´€ë¦¬ ë“±)
    2. ë¸”ë¡ì²´ì¸ ë˜ëŠ” ì•”í˜¸í™”í(ê°€ìƒí™”í)ì— ëŒ€í•´ ì–¸ê¸‰í•˜ê³  ìˆëŠ”ê°€? (ë¹„íŠ¸ì½”ì¸, ì´ë”ë¦¬ì›€, ë””ì§€í„¸ í† í° ë“±)
    3. ë””ì§€í„¸ ê²°ì œ ë°©ì‹ì´ë‚˜ ì‹œìŠ¤í…œì— ëŒ€í•´ ì„¤ëª…í•˜ê³  ìˆëŠ”ê°€? (ì „ìì§€ê°‘, ë¹„ëŒ€ë©´ ê²°ì œ, NFC ê²°ì œ ë“±)
    4. ê¸ˆìœµ ë°ì´í„° ë¶„ì„ì´ë‚˜ ì•Œê³ ë¦¬ì¦˜ íŠ¸ë ˆì´ë”©ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ í¬í•¨í•˜ê³  ìˆëŠ”ê°€? (ë¹…ë°ì´í„° ë¶„ì„, ì¸ê³µì§€ëŠ¥ì„ í™œìš©í•œ íˆ¬ì ì „ëµ ë“±)
    5. ë””ì§€í„¸ ê¸ˆìœµ ê·œì œ ë˜ëŠ” ì •ì±…ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê³  ìˆëŠ”ê°€? (ë””ì§€í„¸ ê¸ˆìœµ ë³´ì•ˆ, ê°œì¸ì •ë³´ ë³´í˜¸ë²•, ì „ìê¸ˆìœµê±°ë˜ë²• ë“±)
    6. ë””ì§€í„¸ í”Œë«í¼ì„ ì´ìš©í•œ ê¸ˆìœµ ì„œë¹„ìŠ¤ë‚˜ ì œí’ˆì„ ë‹¤ë£¨ê³  ìˆëŠ”ê°€? (ë¡œë³´ì–´ë“œë°”ì´ì €, P2P ëŒ€ì¶œ í”Œë«í¼, í¬ë¼ìš°ë“œí€ë”© ë“±)
    7. ë””ì§€í„¸í™”ê°€ ì „í†µ ê¸ˆìœµ ì‚°ì—…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•´ ë…¼ì˜í•˜ê³  ìˆëŠ”ê°€? (ì€í–‰ì˜ ë””ì§€í„¸ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜, ê¸ˆìœµê¸°ê´€ì˜ IT ì¸í”„ë¼ ê°œì„  ë“±)

    ### ë‰´ìŠ¤ ê¸°ì‚¬
    {{content}}

    ### ë‹µë³€
    ë””ì§€í„¸ í•´ë‹¹ì—¬ë¶€(1 ë˜ëŠ” 0):
    """,

    "sentiment" : """ë‹¤ìŒ ê¸°ì‚¬ë¥¼ ë³´ê³  ì•„ë˜ ì •ë³´ë¥¼ "ê¸ì •ë„" í‚¤ ê°’ìœ¼ë¡œ JSON í˜•íƒœë¡œ ì¶”ì¶œí•˜ì„¸ìš”.
     "You are a helpful assistant designed to output JSON."
        ê¸ì •ë„ ì ìˆ˜ë¥¼ 1 ~ 100 ì‚¬ì´ ì ìˆ˜ë¡œ í‘œí˜„í•˜ì„¸ìš”
        ê¸ì •ì ì¸ ê¸°ì‚¬ëŠ” 80ì  ì´ìƒ , ì¤‘ë¦½ì ì¸ ê¸°ì‚¬ëŠ” 60ì  ì´í•˜ 50ì  ì´ìƒ, ë¶€ì •ì ì¸ ê¸°ì‚¬ëŠ” 20ì  ì´í—ˆë¡œ í‘œí˜„í•˜ì„¸ìš”

        #
        ê¸°ì‚¬ë‚´ìš© : {{content}}
        ê¸ì •ë„ :
        """,

    "importance" : """ë‹¹ì‹ ì€ ì „ë¬¸ ê¸°ìì…ë‹ˆë‹¤. ë‹¤ìŒ ê¸°ì‚¬ë¥¼ ë³´ê³  ì•„ë˜ ì •ë³´ë¥¼ "ì¤‘ìš”ë„" í‚¤ ê°’ìœ¼ë¡œ JSON í˜•íƒœë¡œ ì¶”ì¶œí•˜ì„¸ìš”.
     "You are a helpful assistant designed to output JSON."
        ê¸°ì‚¬ì˜ ì¤‘ìš”ë„ë¥¼ ì ìˆ˜ 1ì—ì„œ 2ê¹Œì§€ë¡œ í‘œí˜„(ì¤‘ìš”í• ìˆ˜ë¡ ì ìˆ˜ê°€ ì˜¬ë¼ê°‘ë‚˜ë‹¤)
        ì‹ í•œì€í–‰, ì‹ í•œ ê´€ë ¨ëœ ê¸°ì‚¬ëŠ” ì¤‘ìš”ë„ 2ì„ ë¶€ì—¬í•˜ì„¸ìš”.
        ë””ì§€í„¸ ê´€ë ¨ ë‚´ìš©ì´ë©´ ì¤‘ìš”ë„ 2ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´ AI, í•€í…Œí¬ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¤‘ìš”ë„ 2ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”.
        KBêµ­ë¯¼, êµ­ë¯¼ì€í–‰, í•˜ë‚˜ì€í–‰, í•˜ë‚˜, ìš°ë¦¬ì€í–‰, ìš°ë¦¬, ì¹´ì¹´ì˜¤ë±…í¬, í† ìŠ¤ë±…í¬, ì¼€ì´ë±…í¬ì˜ ë””ì§€í„¸ ì£¼ì œì™€ ê´€ë ¨ëœ ê¸°ì‚¬ëŠ” ì¤‘ìš”ë„ 2ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”
        ì ê¸ˆ ì¶œì‹œ, ì˜ˆê¸ˆ ì¶œì‹œì™€ ê°™ì€ ì‹ ìƒí’ˆì— ê´€ë ¨ëœ ê¸°ì‚¬ëŠ” ì¤‘ìš”ë„ 2ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”.
        ìœ„ ì¡°ê±´ë“¤ì— í•´ë‹¹ë˜ì§€ ì•ŠëŠ” ê¸°ì‚¬ëŠ” ì¤‘ìš”ë„ 1ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”


        # ì˜ˆì‹œ
        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© : ì‹ í•œê¸ˆìœµ, ì €ì¶œì‚° ê·¹ë³µ ìœ„í•œ ëŒ€ì²´ì¸ë ¥ì§€ì›ì‚¬ì—…ì— 100ì–µì› ì¶œì—°
        ì¤‘ìš”ë„  : 2

        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© : ì†Œë¹„ì 2000ëª…ì´ ë¸”ë¼ì¸ë“œ í…ŒìŠ¤íŠ¸ë¡œ ë½‘ì€ ìµœê³ ì˜ íŠ¸ë˜ë¸” ì¹´ë“œëŠ”?
        ì¤‘ìš”ë„ : 2

        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© : í† ìŠ¤ë±…í¬, ì‹ ìš©ë³´ì¦ê¸°ê¸ˆ 'ì´ì§€ì› ëŒ€ì¶œ' ì¶œì‹œ
        ì¤‘ìš”ë„ : 2

        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© : ë˜ ê·œì œë¦¬ìŠ¤í¬ ì§ë©´í•œ ì¹´ì¹´ì˜¤ë±…í¬Â·Â·Â·ì‚¬ì—… ë‹¤ê°í™” â€˜ì ˆì‹¤â€™
        ì¤‘ìš”ë„ : 2

        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© : (5ëŒ€ ì€í–‰ AI ì „ìŸ)â‘¤ë†í˜‘ì€í–‰, ë¹ ë¥¸ ë„ì…ì—ë„ ì„±ê³¼ ë¯¸ì§„â€¦íš¨ìœ¨í™” 'ìˆ™ì œ'
        ì¤‘ìš”ë„ : 2

        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© : ì¹´ë“œìˆ˜ìˆ˜ë£Œ ë‚´ë…„ ë˜ ë‚´ë ¤ê°€ë‚˜â€¦ì¹´ë“œì‚¬ "ì¸í•˜ì—¬ë ¥ ë”ëŠ” ì—†ë‹¤" ë°˜ë°œ â˜…â˜…â˜…
        ì¤‘ìš”ë„ : 2
        
        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© : ê°„í¸ì†¡ê¸ˆ ì•…ìš© ë³´ì´ìŠ¤í”¼ì‹±, 28ì¼ë¶€í„° ì‹ ì†íˆ ì°¨ë‹¨í•œë‹¤ â˜…â˜…â˜…
        ì¤‘ìš”ë„ : 2

        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© : êµ­ë‚´ì€í–‰ ìƒë°˜ê¸° ìˆœì´ìµ 12ì¡°6000ì–µì›â€¦ì „ë…„æ¯” 11%â†“
        ì¤‘ìš”ë„ : 2

        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© : "ë§ë¶„ë¦¬ ê°œì„  ë³´ì•ˆ ëŒ€ì±… ì–´ë–»ê²Œ?" ê¸ˆìœµë‹¹êµ­, å…¨ ê¸ˆìœµì—…ê¶Œ ëŒ€ìƒ ì„¤ëª…íšŒ ê°œìµœ
        ì¤‘ìš”ë„ : 2

        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© : í•€ë‹¤, ë‚´ì§‘ ëŒ€ì¶œí•œë„ ê³„ì‚°ê¸° ì˜¤í”ˆâ€¦LTVÂ·DTI í•œë²ˆì— ê³„ì‚°
        ì¤‘ìš”ë„ : 1 
        
        # ì‹¤ì œ
        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© : {{content}}
        ì¤‘ìš”ë„ : 
        """,
        
    "status"  : 
        """ë‹¹ì‹ ì€ ì „ë¬¸ ê¸°ì ì…ë‹ˆë‹¤. ë‹¤ìŒ ê¸°ì‚¬ë¥¼ ì œëª©ì„ ë³´ê³  ê´€ë ¨ ë‚´ìš©ì´ "ì •ë¶€ì •ì±… ë° ë™í–¥" ê´€ë ¨ ë‚´ìš©ì¸ì§€ "ê¸ˆìœµíšŒì‚¬" ê´€ë ¨ ëœ ë‚´ìš©ì¸ì§€ ë¶„ë¥˜í•˜ì„¸ìš”. 
    ì•„ë˜ ì˜ˆì‹œë¥¼ ì°¸ê³ í•´ì„œ "êµ¬ë¶„"ì´ë¼ëŠ” í‚¤ ê°’ìœ¼ë¡œ JSON í˜•íƒœë¡œ ì¶”ì¶œí•˜ì„¸ìš”. "You are a helpful assistant designed to output JSON."
        ì‹ í•œì€í–‰, KBêµ­ë¯¼, êµ­ë¯¼ì€í–‰, í•˜ë‚˜ì€í–‰, í•˜ë‚˜, ìš°ë¦¬ì€í–‰, ìš°ë¦¬, ì¹´ì¹´ì˜¤ë±…í¬, í† ìŠ¤ë±…í¬, ì¼€ì´ë±…í¬ ê´€ë ¨ ê¸°ì‚¬ëŠ”  "ê¸ˆìœµíšŒì‚¬"ì— í•´ë‹¹ë˜ë©°
       ê¸ˆìœµê°ë…ì›, ê¸ˆê°ì›, ì •ë¶€ë¶€ì²˜, ì •ì±…, ì‹œì¤‘ì€í–‰, ì¸ë±…3ì‚¬ ê´€ë ¨ ê¸°ì‚¬ëŠ” "ì •ë¶€ì •ì±… ë° ë™í–¥"ì— í•´ë‹¹ ë©ë‹ˆë‹¤.
       
        # ì˜ˆì‹œ
        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© : BCì¹´ë“œ, í•œêµ­ ê¸ˆìœµ ìµœì í™”ëœ AI ë¬´ìƒ ê³µê°œ
        ì˜ˆì‹œ ë¶„ë¥˜ : ê¸ˆìœµíšŒì‚¬

        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© :  í‹°ëª¬,ìœ„ë©”í”„ì—..ì€í–‰ê¶Œ ì„ ì •ì‚°ëŒ€ì¶œ ì·¨ê¸‰ ì¤‘ë‹¨
        ì˜ˆì‹œ ë¶„ë¥˜ : ê¸ˆìœµíšŒì‚¬

        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© : í† ìŠ¤ë±…í¬-í•˜ë‚˜ì¹´ë“œ í•¨ê»˜ ì‹ ìš©ì¹´ë“œ ë§Œë“ ë‹¤. PLCCì—…ë¬´í˜‘ì•½ ì²´ê²° 
        ì˜ˆì‹œ ë¶„ë¥˜ : ê¸ˆìœµíšŒì‚¬

        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© :  ìš°ë¦¬ì€í–‰, ITì„œë¹„ìŠ¤ê´€ë¦¬ êµ­ì œí‘œì¤€ ì¸ì¦ íšë“
        ì˜ˆì‹œ ë¶„ë¥˜ : ê¸ˆìœµíšŒì‚¬
    
        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© : "ë§ë¶„ë¦¬ ê°œì„  ë³´ì•ˆ ëŒ€ì±… ì–´ë–»ê²Œ?" ê¸ˆìœµë‹¹êµ­, å…¨ ê¸ˆìœµì—…ê¶Œ ëŒ€ìƒ ì„¤ëª…íšŒ ê°œìµœ
        ì˜ˆì‹œ ë¶„ë¥˜ : ì •ë¶€ì •ì±… ë° ë™í–¥

        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© : ê¸ˆê°ì›, ê°€ìƒìì‚°ê±°ë˜ì†Œ ì˜ˆì¹˜ê¸ˆ ì´ììœ¨ ê³µí†µê¸°ì¤€ ë§ˆë ¨ ìš”êµ¬
        ì˜ˆì‹œ ë¶„ë¥˜ : ì •ë¶€ì •ì±… ë° ë™í–¥

        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© : ê·œì œìƒŒë“œë°•ìŠ¤ ì „ìš©í€ë“œ 175ì–µì› ì¡°ì„± 'ì‹ ì‹œì¥ì°½ì¶œ'
        ì˜ˆì‹œ ë¶„ë¥˜ : ì •ë¶€ì •ì±… ë° ë™í–¥
        
        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© : ì¸í„°ë„·ì€í–‰ë„ ì¤‘,ì €ì‹ ìš©ì ëŒ€ì¶œ ì¤„ì˜€ë‹¤
        ì˜ˆì‹œ ë¶„ë¥˜ : ì •ë¶€ì •ì±… ë° ë™í–¥

        ì˜ˆì‹œ ê¸°ì‚¬ ì œëª© : êµ­ë‚´ì€í–‰ ìƒë°˜ê¸° ìˆœì´ìµ 12ì¡°6000ì–µì›â€¦ì „ë…„æ¯” 11%â†“
        ì˜ˆì‹œ ë¶„ë¥˜ : ì •ë¶€ì •ì±… ë° ë™í–¥

        # ì‹¤ì œ
        ê¸°ì‚¬ ë‚´ìš© : {{content}}
        êµ¬ë¶„ :
        """,

    "duplicate" : "",
    
    "summary" : 
    """ë‹¹ì‹ ì˜ ì—…ë¬´ëŠ” ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê°„ëµí•˜ê²Œ ìš”ì•½í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    ìš”ì•½ë¬¸ì„ "ìš”ì•½ë¬¸" í‚¤ ê°’ìœ¼ë¡œ JSON í˜•íƒœë¡œ ì¶”ì¶œí•˜ì„¸ìš”.
    "You are a helpful assistant designed to output JSON."

    1. ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì½ê³ , í•µì‹¬ ë‚´ìš©ì„ 3ì¤„ ì´ë‚´ë¡œ ê°„ë‹¨í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
    2. ìš”ì•½ë¬¸ì„ ì¡´ëŒ“ë§ë¡œ í•´ì£¼ì„¸ìš”.

    ### ë‰´ìŠ¤ ê¸°ì‚¬
    {{content}}

    ### ë‹µë³€
    ìš”ì•½ë¬¸(ì¡´ëŒ“ë§ë¡œ):
    """
    ,

}



class main_bot():
    def __init__(self, load_option, date="", custom_url=""):
        print("Initialized!")
        self.load_option = load_option
        #self.credential = credential
        self.credential = st.secrets
        self.key = self.credential['chatgpt']['api_key']
        self.client = OpenAI(api_key=self.key)
        self.custom_url=custom_url

        if not date :
            self.target_date = datetime.strptime( (datetime.today() +  timedelta(days=1)).strftime('%Y%m%d'), "%Y%m%d")
        else : 
            self.target_date = datetime.strptime(date, "%Y%m%d")

        # 0. Collect + short url
        if not self.load_option :
            self.news_df = self.collect()
        else :
            self.load()
            #### For test
            #self.news_df = self.news_df[:10] ########### Temp


    def run(self):
        progress_text = "ë””ì§€í„¸ ê¸°ì‚¬ ì¶”ì¶œ"
        my_bar = st.progress(0, text=progress_text)
        
        # 1. Check Digital
        self.news_df = self.check_digital()
        my_bar.progress(5, text="ê¸ì •ë„ ë¶„ì„") 
        
        # 2. Sentiment
        self.news_df['sentiment'] = self.get_sentiment()
        my_bar.progress(15, text="ì¤‘ìš”ë„ ë¶„ì„")     
           
        # 3. importance & sort
        self.get_importance()
        my_bar.progress(30, text="ë™í–¥ ë¶„ë¥˜")        

        # 4. Classify - Status/ Trend 
        self.news_df['class'] = self.get_status()
        my_bar.progress(45, text="ì¤‘ë³µ ì œê±°")        

        # 5. Remove Duplicate
        self.news_df = self.remove_duplicate()
        my_bar.progress(80, text="ìš”ì•½")        

        # 6. Summary
        self.news_df["summary"] = self.get_summary()

        # 7. Template
        my_bar.progress(99, text="ìµœì¢…ì¶œë ¥")        
        # 7.1 digital study
        self.digital_study =  self.get_study()
        response = self.digitaldaily_format()
        self.digitaldaily_format_all() # Temp

    def collect(self):
        
        if self.custom_url :
            with open("./urls.txt", "r") as f :
                lines = f.readlines()
            lines = [line.replace('\n', '') for line in lines]
            urls = lines
        else :
            urls = get_naverlinks(url="https://news.naver.com/breakingnews/section/101/259?date={}".format((self.target_date -  timedelta(days=1)).strftime('%Y%m%d')))
        news_list = [fetch_article_details(url) for url in urls]
        news_df = pd.DataFrame(news_list)
        # Remove no contents articles
        news_df = news_df[news_df['content'].apply(lambda x : len(x) != 0)]
        news_df = news_df.reset_index(drop=True)
        news_df = self.quick_remove_duplicate(news_df)
        news_df["shorturl"] = news_df["url"].map(lambda x : get_shorturl(x,self.credential))
        return news_df 
    
    def load(self):
        # 8ì›” 3ì¼ ë°œì†¡
        # 8ì›” 2ì¼ ì „ì²´

        data_date = (self.target_date  -  timedelta(days=1)).strftime('%Y%m%d')
        load_path = "./data/news_df_" + data_date  + ".csv"
        self.news_df = pd.read_csv(load_path)
        idxs = self.news_df['time'].map(lambda x : check_date(x, (self.target_date  -  timedelta(days=1)).strftime('%Y.%m.%d.')))
        self.news_df = self.news_df[idxs]
        self.news_df = self.news_df.dropna().reset_index(drop=True)
        # ë¹ ë¥¸ ì¤‘ë³µì œê±°ë¥¼ ìœ„í•œ TFIDF
        self.news_df = self.quick_remove_duplicate(self.news_df)
    
    def quick_calculate_similarity(self, article1, article2):
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform([article1, article2])
        cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])
        return cosine_sim[0][0]
    
    def quick_remove_duplicate(self, news_df):
        news_pair = list(itertools.combinations(range(len(news_df)), 2))
        news_duplicate = []
        for (i, j) in news_pair:
            cos_sim = self.quick_calculate_similarity(news_df.loc[i, 'content'], news_df.loc[j, 'content'])
            if cos_sim >= 0.1:
                news_duplicate.append(j)
        news_unique = news_df.drop(list(set(news_duplicate)))
        return news_unique.reset_index(drop = True)
        
    def check_digital(self):
        news_df = self.news_df
        
        system_prompt = prompt_templates["system"] 
        environment = jinja2.Environment()
        jinja_template = environment.from_string(prompt_templates["digital"])

        digital_yn = []

        for content in news_df['content'] :
            prompt = jinja_template.render(content=content)
            response = self.client.chat.completions.create(
            model="gpt-3.5-turbo-0125",
            response_format={ "type": "json_object" },
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}]
            )
            try : 
                digital_yn.append((eval(response.choices[0].message.content)["ë””ì§€í„¸"]))
            except :
                digital_yn.append(-1)
                pass
            
        print("Check digital")
        
        news_df['digital_yn'] = digital_yn
        digital_news_df = news_df[news_df['digital_yn'] == 1].reset_index(drop = True)

        return digital_news_df 

    def get_sentiment(self):
        self.news_df = self.news_df
        news_df = self.news_df

        system_prompt = prompt_templates["system"] 
        environment = jinja2.Environment()
        jinja_template = environment.from_string(prompt_templates["sentiment"])

        scores = []

        for content in  news_df['content'] :
            prompt = jinja_template.render(content=content)
            response = self.client.chat.completions.create(
            model="gpt-3.5-turbo-0125",
            response_format={ "type": "json_object" },
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}]
            )
            try : 
                scores.append((eval(response.choices[0].message.content)["ê¸ì •ë„"]))
            except :
                scores.append(-1)
                pass
        print("Sentiment test")
        return scores

    def get_importance(self):
        news_df = self.news_df
        print(news_df)
        system_prompt = prompt_templates["system"]
        environment = jinja2.Environment()
        jinja_template = environment.from_string(prompt_templates["importance"])

        scores = []

        for title in  news_df['title'] :
            prompt = jinja_template.render(content=title)
            response = self.client.chat.completions.create(
            model="gpt-3.5-turbo-0125",
            response_format={ "type": "json_object" },
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}]
            )
            try : 
                scores.append((eval(response.choices[0].message.content)["ì¤‘ìš”ë„"]))
            except :
                scores.append(1)
                pass
        self.news_df['importance'] = scores

        # Importanceê°€ 2ì¸ë° ì‹ í•œ ë“¤ì–´ê°€ ìˆìœ¼ë©´ 3ì ìœ¼ë¡œ ë°”ê¾¸ëŠ” ë¡œì§
        scores = []
        for record in self.news_df.to_dict('records') :
            title, score = record['title'], record['importance']
            if ("í† ìŠ¤" in title) or ("KB" in title) or ("ìš°ë¦¬ì€í–‰" in title) or ("í•˜ë‚˜ì€í–‰" in title) or \
             ("ê¸ˆê°ì›" in title) or ("ê¸ˆìœµìœ„" in title) or ("ì´ë³µí˜„" in title):
                score = min(score + 1 , 2)
            if ("ì‹ í•œ" in title) or ("ì‹ í•œì€í–‰" in title) :
                score = min(score + 1 , 3)
            
            
            scores.append(score)
        self.news_df['importance']=scores
        self.news_df.sort_values(by=['importance', 'sentiment'], axis=0, ascending=False, inplace=True)
        print("importance test")
        return scores

    def get_status(self):
        news_df = self.news_df
        print(news_df)
        system_prompt = prompt_templates["system"]
        environment = jinja2.Environment()
        jinja_template = environment.from_string(prompt_templates["status"])

        scores = []

        for content in  news_df['title'] :
            prompt = jinja_template.render(content=content)
            response = self.client.chat.completions.create(
            model="gpt-3.5-turbo-0125",
            response_format={ "type": "json_object" },
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}]
            )
            try : 
                scores.append((eval(response.choices[0].message.content)["êµ¬ë¶„"]))
            except :
                scores.append("null")
                pass
        print("status test")
        return scores
    
    def get_embedding_db(self):
        news_df = self.news_df 
        embedding_array = []
        for idx in range(len(news_df)):
            article = news_df.loc[idx, 'content']
            embedding = np.array(self.get_embedding(article)).reshape(1, -1)
            embedding_array.append(embedding)
        embedding_array = np.vstack(embedding_array)
        self.embedding_array = embedding_array
        

    def get_embedding(self, text):
        response = self.client.embeddings.create(input=[text], model="text-embedding-ada-002")
        return response.data[0].embedding

    def calculate_similarity(self, article1, article2):
        embedding1 = self.embedding_array[article1].reshape(1, -1)
        embedding2 = self.embedding_array[article2].reshape(1, -1)
        cosine_sim = cosine_similarity(embedding1, embedding2)
        return cosine_sim[0][0]
    
    def remove_duplicate(self, threshold = 0.9):
        self.get_embedding_db()
        news_df = self.news_df
        news_pair = list(itertools.combinations(range(len(news_df)), 2))
        news_duplicate = []

        for (i, j) in news_pair:
            cos_sim = self.calculate_similarity(i, j)
            
            if cos_sim >= threshold:
                news_duplicate.append(j)
        
        news_unique = news_df.drop(list(set(news_duplicate)))
        return news_unique.reset_index(drop = True)

    def get_summary(self):
        news_df = self.news_df
        
        system_prompt = prompt_templates["system"]
        environment = jinja2.Environment()
        jinja_template = environment.from_string(prompt_templates["summary"])

        summary = []

        for i in range(len(news_df)) :
            prompt = jinja_template.render(content = news_df.loc[i, 'content'])
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo-0125",
                response_format={"type": "json_object"},
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}]
            )
            try : 
                summary.append((eval(response.choices[0].message.content)["ìš”ì•½ë¬¸"]))
            except :
                summary.append("null")
                pass
                
        return summary
    
    def get_study(self):
        articles = get_study()
        self.study = get_study_format(articles, self.credential)


    def digitaldaily_format(self):
        # Key value check
        for key in ["publisher", "url"] :
            if key not in self.news_df.columns:
                self.news_df[key] = 'temp'

        status = get_output_format(self.news_df, status="ê¸ˆìœµíšŒì‚¬")
        trend = get_output_format(self.news_df, status="ì •ë¶€ì •ì±… ë° ë™í–¥")
        study = self.study

        ####### To Do
        response = get_jinga(status, trend, study, target_date=self.target_date)
        print(response)
        with open("./data/result.txt", "a") as f:
            f.write(response)
        with st.chat_message("assistant"):
            st.code(response)

        return response
    def digitaldaily_format_all(self):
        # temp for recording
        for key in ["publisher", "url"] :
            if key not in self.news_df.columns:
                self.news_df[key] = 'temp'

        status = get_output_format(self.news_df, 
                                   status="ê¸ˆìœµíšŒì‚¬", max_num=-1)
        trend = get_output_format(self.news_df, 
                                  status="ì •ë¶€ì •ì±… ë° ë™í–¥", max_num=-1)
        study = self.study

        ####### To Do
        # Save
        response = get_jinga(status, trend, study, target_date=self.target_date)
        with open("./data/result_all.txt", "w") as f:
            f.write(response)
        self.news_df.to_csv("news_df_{}".format(datetime.today().strftime('%Y%m%d')),index=False)

        expander = st.expander(label = "Advanced tools ğŸ› ï¸")
        with expander :
            cols= st.columns((1,1))
            cols[0].download_button("ì „ì²´ ê¸°ì‚¬ ë‹¤ìš´ë¡œë“œ", response, file_name="ì „ì²´ê¸°ì‚¬.txt")
            cols[1].button("ìƒˆë¡œê³ ì¹¨")
            

        return response

# Parsing bot
class parse_bot():
    def __init__(self, save_date='', save_path="../data/news_df_") :
        self.credential = st.secrets
        self.key = credential['chatgpt']['api_key']
        self.fpath = save_path
        self.isexist_flag = False
        if save_date == '' :
            self.save_date = datetime.today().strftime('%Y%m%d')
        else :
            self.save_date = save_date
        # 1. Collect + short url
        self.collect_keywords = ["ì •ìƒí˜", "ì§„ì˜¥ë™", "ì€í–‰ì¥", 
        "ì‹ í•œì€í–‰", "ì‹ í•œ", "KBêµ­ë¯¼", "êµ­ë¯¼ì€í–‰", "í•˜ë‚˜ì€í–‰", "í•˜ë‚˜", "ìš°ë¦¬ì€í–‰", "ìš°ë¦¬", "ê¸ˆìœµì§€ì£¼", 
         "ì¹´ì¹´ì˜¤ë±…í¬", "í† ìŠ¤ë±…í¬", "ì¼€ì´ë±…í¬", "í† ìŠ¤", "ë„¤ì´ë²„", "ì¹´ì¹´ì˜¤","ë±…í¬ìƒëŸ¬ë“œ",
          "ê¸ˆìœµìœ„", "ë§ë¶„ë¦¬", "ê¸ˆìœµê°ë…ì›", "ê¸ˆê°ì›", "ì •ë¶€ë¶€ì²˜", "ì •ì±…", 
          "ì‹œì¤‘ì€í–‰", "ì¸ë±…3ì‚¬", "ì´ë³µí˜„", "í•€í…Œí¬",
          "ì „ì„¸ëŒ€ì¶œ", "ì£¼ë‹´ëŒ€",
          "ê¸ˆìœµê¶Œ", "í•œì€", "í•œêµ­ì€í–‰"]
        self.non_collect_keywords = ["ì±„ìš©"]
        self.news_df =   self.collect()
        self.client = OpenAI(api_key=self.key)


    
    def run(self):
        # Collect news date every hour 
        for i in range(144) :
            try : 
                new_df = self.collect()
                old_df = self.news_df 
                idxs = []
                for idx, value in enumerate(new_df.url) :
                    if value not in old_df.url.values :
                        idxs.append(idx)

                new_df = pd.concat([old_df, new_df.iloc[idxs]], ignore_index=True)
                self.news_df = new_df
                self.save()
                print("Saved time : ",  self.save_date,  i)
            except : 
                print("Error : ", datetime.now())
            time.sleep(3600) 

    def collect(self):
        urls = get_naverlinks(url="https://news.naver.com/breakingnews/section/101/259?date={}".format(self.save_date))
        news_list = [fetch_article_details(url) for url in urls]
        news_df = pd.DataFrame(news_list)
        TF = news_df.content.map(lambda x : True if any(keyword in x for keyword in self.collect_keywords) else False)
        news_df = news_df[TF]
        TF = news_df.content.map(lambda x : False if any(keyword in x for keyword in self.non_collect_keywords) else True)
        news_df = news_df[TF]
        news_df["shorturl"] = news_df["url"].map(lambda x : get_shorturl(x,self.credential))
        return news_df 
    
    def save(self):
        fname = self.fpath +  self.save_date + ".csv"
        if self.isexist_flag :
            self.news_df.to_csv(fname, index=False)
        else :
            # Check previous 
            if os.path.isfile(fname):
                old_df = pd.read_csv(fname)
                new_df = self.news_df 
                idxs = []

                for idx, value in enumerate(new_df.url) :
                    if value not in old_df.url.values :
                        idxs.append(idx)

                new_df = pd.concat([old_df, new_df.iloc[idxs]], ignore_index=True)
                self.news_df = new_df
                self.isexist_flag = True 
                self.news_df.to_csv(fname, index=False)
            else : # 24 ì‹œê°„ì´ ì§€ë‚˜ì„œ ìƒˆë¡œìš´ íŒŒì¼ ìƒì„± í•´ì•¼ë  ë•Œ
                self.news_df.to_csv(fname, index=False)

            
            

if __name__ == "__main__":
    bot = main_bot()
    #bot = parse_bot(cred_path)
    bot.run()
